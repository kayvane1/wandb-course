{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weights & Biases - Week 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will be using the Complaints Dataset available on the HuggingFace Hub to build a classification model. \n",
    "\n",
    "The (fictional) business case of this model is that a corporation wants to use a classifier to help route complaints to the right departments and teams to improve the customer journey. The complaints department has collected a large amount of complaints and which departments dealt with them, highlighting the most probable issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by importing the required libraries to run this example notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch import tensor, nn, device, cuda\n",
    "from transformers import AutoTokenizer, TrainingArguments, Trainer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from transformers.trainer_callback import EarlyStoppingCallback, TrainerCallback\n",
    "from huggingface_hub import HfFolder\n",
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by Logging into HF Hub and Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Login to HF Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up our Weights and Biases Parameters at the start so we can have them easily accessible in one place if we want to change the way our data lineage dag looks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WANDB BASE PARAMETERS\n",
    "PROJECT_NAME = 'wandb-week-1-complaints-classifier'\n",
    "ENTITY = \"wandb_course\"\n",
    "# WANDB ARTIFACT TYPES\n",
    "DATASET_TYPE = \"dataset\"\n",
    "MODEL_TYPE = \"model\"\n",
    "# WANDB JOB TYPES\n",
    "RAW_DATA_JOB_TYPE = \"fetch-raw-data\"\n",
    "DATA_PROCESSING_JOB_TYPE = \"process-data\"\n",
    "SPLIT_DATA_JOB_TYPE = \"split-data\"\n",
    "MODEL_TRAINING_JOB_TYPE = \"model-training\"\n",
    "# WANDB ARTIFACT NAMES\n",
    "RAW_DATA_ARTIFACT = \"complaints_raw_data\"\n",
    "PROCESSED_DATA_ARTIFACT = \"complaints_raw_data\"\n",
    "TRAIN_DATA_ARTIFACT = \"complaints_train_data\"\n",
    "TEST_DATA_ARTIFACT = \"complaints_test_data\"\n",
    "# DATA FOLDERS\n",
    "RAW_DATA_FOLDER = 'complaints-dataset/raw'\n",
    "PROCESSED_DATA_FOLDER = 'complaints-dataset/processed'\n",
    "TRAIN_DATA_FOLDER = 'complaints-dataset/train'\n",
    "TEST_DATA_FOLDER = 'complaints-dataset/test'\n",
    "MODEL_DATA_FOLDER = 'complaints-model'\n",
    "# DATASET COLUMNS TO KEEP\n",
    "TEXT_COLUMN = \"Complaints Text\"\n",
    "TARGET_COLUMN = \"Sub Product\"\n",
    "# TRANSFORMERS PARAMETERS\n",
    "MODEL_NAME = \"distilbert-base-uncased\"\n",
    "NUM_EPOCHS = 3\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "EVAL_BATCH_SIZE = 32\n",
    "WARMUP_STEPS = 500\n",
    "LEARNING_RATE = 5e-5\n",
    "FP16 = True\n",
    "# HUB PARAMETERS\n",
    "PUSH_TO_HUB = True\n",
    "HUB_MODEL_ID = \"distilbert-complaints-wandb\"\n",
    "HUB_STRATEGY = \"every_save\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download & Log Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=RAW_DATA_JOB_TYPE)\n",
    "\n",
    "# Loading consumer complaints dataset - Note: This is a big dataset\n",
    "text_dataset = load_dataset(\"consumer-finance-complaints\", ignore_verifications=True)\n",
    "\n",
    "# Create and log the raw data artifact\n",
    "raw_data_art = wandb.Artifact(text_dataset, type=DATASET_TYPE)\n",
    "raw_data_art.add_dir(RAW_DATA_FOLDER, name=RAW_DATA_ARTIFACT)\n",
    "run.log_artifact(raw_data_art)\n",
    "\n",
    "# end the RAW DATA job\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process & Log Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up the tokenizer and encoder function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Used to encode the labels\n",
    "label2id = text_dataset['train'].features[TARGET_COLUMN].str2int()\n",
    "\n",
    "# Used later to initialise the model\n",
    "number_classes = text_dataset['train'].features['labels'].num_classes\n",
    "id2label = text_dataset['train'].features[TARGET_COLUMN].int2str()\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    tokenized_batch = tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "    tokenized_batch[\"labels\"] = [label2id[label] for label in batch[\"labels\"]]\n",
    "    return tokenized_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the data as part of a wandb run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=DATA_PROCESSING_JOB_TYPE)\n",
    "\n",
    "# By including `use_artifact` we're logging the usage to W&B and can track it as part of the lineage\n",
    "text_dataset = run.use_artifact(f'{RAW_DATA_ARTIFACT}:latest')\n",
    "\n",
    "# Extracting the target column, there is only one split at this point (train)\n",
    "columns = text_dataset['train'].column_names\n",
    "\n",
    "# Remove the columns which aren't in scope for us\n",
    "remove_cols = [e for e in columns if e not in (TEXT_COLUMN, TARGET_COLUMN)]\n",
    "processed_data = text_dataset.remove_columns(remove_cols)\n",
    "\n",
    "# Renaming the columns to the names expected by the classifier\n",
    "processed_data = processed_data.rename_column(TEXT_COLUMN, \"text\")\n",
    "processed_data = processed_data.rename_column(TARGET_COLUMN, \"labels\")\n",
    "\n",
    "# Filtering out empty/no-text complaints\n",
    "processed_data = processed_data.filter(lambda example: len(example['text'])>0)\n",
    "\n",
    "# tokenize dataset\n",
    "processed_data = processed_data['train'].map(tokenize, batched=True)\n",
    "\n",
    "# set format for pytorch\n",
    "processed_data.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "# Create and log the raw data artifact\n",
    "processed_data_art = wandb.Artifact(processed_data, type=DATASET_TYPE)\n",
    "processed_data_art.add_dir(PROCESSED_DATA_FOLDER, name=PROCESSED_DATA_ARTIFACT)\n",
    "run.log_artifact(processed_data_art)\n",
    "\n",
    "# End the PROCESS DATA job\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=SPLIT_DATA_JOB_TYPE)\n",
    "\n",
    "# By including `use_artifact` we're logging the usage to W&B and can track it as part of the lineage\n",
    "processed_data = run.use_artifact(f'{PROCESSED_DATA_ARTIFACT}:latest')\n",
    "\n",
    "# Splitting the dataset into training and validation datasets\n",
    "split_data = processed_data['train'].train_test_split(test_size=0.2,seed=0)\n",
    "train_dataset = split_data['train']\n",
    "test_dataset = split_data['test']\n",
    "\n",
    "# Create and log the train data artifact\n",
    "train_data_art = wandb.Artifact(processed_data, type=DATASET_TYPE)\n",
    "train_data_art.add_dir(TRAIN_DATA_FOLDER, name=TRAIN_DATA_ARTIFACT)\n",
    "run.log_artifact(train_data_art)\n",
    "\n",
    "# Create and log the test data artifact\n",
    "test_data_art = wandb.Artifact(processed_data, type=DATASET_TYPE)\n",
    "test_data_art.add_dir(TEST_DATA_FOLDER, name=TEST_DATA_ARTIFACT)\n",
    "run.log_artifact(test_data_art)\n",
    "\n",
    "# End the SPLIT DATA job\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set-Up Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-Up the Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "output_dir=MODEL_DATA_FOLDER.as_posix(),\n",
    "num_train_epochs=NUM_EPOCHS,\n",
    "per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "warmup_steps=WARMUP_STEPS,\n",
    "fp16=FP16,\n",
    "learning_rate=float(LEARNING_RATE),\n",
    "# logging & evaluation strategies\n",
    "logging_dir=f\"{MODEL_DATA_FOLDER.as_posix()}/logs\",\n",
    "logging_steps=50, \n",
    "evaluation_strategy=\"steps\",\n",
    "eval_steps=2000,\n",
    "save_strategy=\"steps\",\n",
    "save_steps=2000,\n",
    "save_total_limit=2,\n",
    "load_best_model_at_end=True,\n",
    "metric_for_best_model=\"f1\",\n",
    "report_to=\"wandb\",\n",
    "# push to hub parameters\n",
    "push_to_hub=PUSH_TO_HUB,\n",
    "hub_strategy=HUB_STRATEGY,\n",
    "hub_model_id=HUB_MODEL_ID,\n",
    "#hub_token=args.hub_token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-Up Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_metrics(eval_pred):\n",
    "  # define metrics and metrics function\n",
    "  f1_metric = load_metric(\"f1\")\n",
    "  accuracy_metric = load_metric( \"accuracy\")\n",
    "  recall_metric = load_metric(\"recall\")\n",
    "  precision_metric = load_metric(\"precision\")\n",
    "  \n",
    "  predictions, labels = eval_pred\n",
    "  predictions = np.argmax(predictions, axis=1) # predictions.argmax(-1)\n",
    "  acc = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "  recall = recall_metric.compute(predictions=predictions, references=labels, average='weighted')\n",
    "  f1 = f1_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "  precision = precision_metric.compute(predictions=predictions, references=labels, average=\"weighted\")\n",
    "\n",
    "  return {\n",
    "      \"accuracy\": acc[\"accuracy\"],\n",
    "      \"f1\": f1[\"f1\"],\n",
    "      \"recall\": recall[\"recall\"],\n",
    "      \"precision\" : precision[\"precision\"]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a Callback to Log Validation Examples to Weights and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SamplesTableLogger(TrainerCallback):\n",
    "    def __init__(self, sample):\n",
    "        super().__init__()\n",
    "\n",
    "        self.dataset = sample\n",
    "\n",
    "    def on_epoch_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "    \n",
    "        val_batch = next(iter(self.datamodule.val_dataloader()))\n",
    "        ids = val_batch[\"input_ids\"]\n",
    "        p_o_t = labels = val_batch[\"piece_of_text_id\"]\n",
    "        del val_batch[\"piece_of_text_id\"]\n",
    "        sentences = tokenizer.batch_decode(ids, skip_special_tokens=True)\n",
    "\n",
    "        labels = val_batch[\"labels\"]\n",
    "        del val_batch[\"labels\"]\n",
    "        val_batch.to(device)\n",
    "        \n",
    "        outputs = model(**val_batch)\n",
    "        logits = outputs.logits\n",
    "        preds = logits.argmax(-1)\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\"Sentence\": sentences, \"Label\": labels, \"Predicted\": preds.cpu().detach().numpy()}\n",
    "        )\n",
    "        \n",
    "        print(df)\n",
    "            \n",
    "        wrong_df = df[df[\"Label\"] != df[\"Predicted\"]]\n",
    "        trainer.logger.experiment.log(\n",
    "            {\n",
    "                \"examples\": wandb.Table(dataframe=wrong_df, allow_mixed_types=True),\n",
    "                \"global_step\": trainer.global_step,\n",
    "            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=PROJECT_NAME, entity=ENTITY, job_type=MODEL_TRAINING_JOB_TYPE)\n",
    "\n",
    "# By including `use_artifact` we're logging the usage to W&B and can track it as part of the lineage\n",
    "train_dataset = run.use_artifact(f'{TRAIN_DATA_ARTIFACT}:latest')\n",
    "test_dataset = run.use_artifact(f'{TEST_DATA_ARTIFACT}:latest')\n",
    "\n",
    "# define data_collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "MODEL_NAME, num_labels=number_classes, label2id=label2id, id2label=id2label\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "outputs = trainer.evaluate()\n",
    "trainer.save_model()\n",
    "\n",
    "# subset the validation dataframe\n",
    "# \n",
    "\n",
    "predictions_df = trainer.predict()\n",
    "\n",
    "\n",
    "run.log({'Evaluation Metrics': wandb.Table(dataframe=pd.DataFrame(outputs))})\n",
    "\n",
    "wandb.finish()\n",
    "\n",
    "# save best model, metrics and create model card\n",
    "if args.push_to_hub:\n",
    "    trainer.create_model_card(model_name=HUB_MODEL_ID)\n",
    "    # wait for asynchronous pushes to finish\n",
    "    time.sleep(180)\n",
    "    trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_training_data(text_column:str, target_column:str):\n",
    "\n",
    "  \"\"\"\n",
    "  Fetches the Complaints dataset from the HuggingFace Hub and generates id and class mappings\n",
    "  \"\"\"  \n",
    "\n",
    "  # Splitting the dataset into training and validation datasets\n",
    "  text_dataset = text_dataset['train'].train_test_split(test_size=0.2,seed=0)\n",
    "\n",
    "  # Extracting the target column\n",
    "  columns = text_dataset['train'].column_names\n",
    "  \n",
    "  # Remove the columns which aren't in scope for us\n",
    "  remove_cols = [e for e in columns if e not in (text_column, target_column)]\n",
    "  text_dataset = text_dataset.remove_columns(remove_cols)\n",
    "\n",
    "  # Isn't this just:\n",
    "  class_mapping = text_dataset['train'].features[target_column].int2str()\n",
    "  label2id = text_dataset['train'].features[target_column].str2int()\n",
    "\n",
    "#   class_mapping = { \n",
    "#       idx: text_dataset['train'].features[target_column].int2str(idx)\n",
    "#       for idx, names in enumerate(\n",
    "#           text_dataset['train'].features[target_column].names)\n",
    "#   }\n",
    "  \n",
    "#   label2id = {\n",
    "#       text_dataset['train'].features[target_column].int2str(idx): idx\n",
    "#       for idx, names in enumerate(\n",
    "#           text_dataset['train'].features[target_column].names)\n",
    "#   }\n",
    "\n",
    "  text_dataset = text_dataset.rename_column(text_column, \"text\")\n",
    "  text_dataset = text_dataset.rename_column(target_column, \"labels\")\n",
    "\n",
    "  number_classes = text_dataset['train'].features['labels'].num_classes\n",
    "\n",
    "  # Filtering out empty/no-text complaints\n",
    "  text_dataset = text_dataset.filter(lambda example: len(example['text'])>0)\n",
    "\n",
    "  return text_dataset, class_mapping, label2id, number_classes\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='Train new HuggingFace Model for a given two column dataset')\n",
    "    parser.add_argument('--text_col', dest='text_col', type=str, help='The name of the text column in the dataset')\n",
    "    parser.add_argument('--feature_col', dest='feature_col', type=str, help='The name of the text column in the dataset')\n",
    "    parser.add_argument('--model_id', dest='model_id', type=str, default='distilbert-base-uncased', help='Name of the HF model to use')\n",
    "    parser.add_argument('--dataset_id', dest='dataset_id', type=str, default='consumer-complaints', help='Name of the HF Hub dataset to use')\n",
    "    parser.add_argument('--experiment_name', dest='experiment_name', default='complaints_dataset', type=str, help='path to the csv file')\n",
    "    parser.add_argument('--from_checkpoint', dest='from_checkpoint', type=str, help='If training should start from a specific checkpoint')\n",
    "    parser.add_argument('--evaluate', dest='evaluate', type=str, help='If training should evaluate the final model performance')\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train_batch_size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval_batch_size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--fp16\", type=bool, default=True)\n",
    "\n",
    "    # Push to Hub Parameters\n",
    "    parser.add_argument(\"--push_to_hub\", type=bool, default=True)\n",
    "    parser.add_argument(\"--hub_model_id\", type=str, default=None)\n",
    "    parser.add_argument(\"--hub_strategy\", type=str, default=\"every_save\")\n",
    "    parser.add_argument(\"--hub_token\", type=str, default=None)\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Set up logging\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.getLevelName(\"INFO\"),\n",
    "        handlers=[logging.StreamHandler(sys.stdout)],\n",
    "        format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    )\n",
    "    \n",
    "    # Check for GPU\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Load dataset\n",
    "    text_dataset, id2label, label2id, number_classes = get_training_data(args.text_col, args.feature_col)\n",
    "\n",
    "    logging.info(\"Analysing Label Distribution\")\n",
    "    train_dataset = text_dataset['train'].to_pandas()\n",
    "    label_info = LabelAnalyser(train_dataset)\n",
    "    logging.info(label_info)\n",
    "\n",
    "    # If the dataset is imbalanced, we want to generate class weights for the loss function to penalise more for rarer classes\n",
    "    if label_info.label_distribution != 'balanced':\n",
    "      logging.info(\"Dataset is imbalanced, generating class weights and undersampling majority class\")\n",
    "      # Sampling majority to the mean value\n",
    "      target_classes, other_classes = sampling_strategy(train_dataset['text'],train_dataset['labels'],round(train_dataset.labels.value_counts().mean()),t='majority')\n",
    "      df = undersample_df(train_dataset,target_classes,other_classes)\n",
    "      logging.info(\"Recalculating label distribution\")\n",
    "      label_info = LabelAnalyser(df)\n",
    "      logging.info(label_info)\n",
    "      weights = label_info.generate_class_weights()\n",
    "      # The weights need to be converted to a torch tensor and loaded into the GPU to be accessible by the Trainer\n",
    "      allocated_weights = tensor(weights)\n",
    "      weights = allocated_weights.to(device)\n",
    "    else:\n",
    "      weights = None\n",
    "\n",
    "    # download tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.model_id)\n",
    "\n",
    "    # tokenizer helper function\n",
    "    def tokenize(batch):\n",
    "        tokenized_batch = tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "        tokenized_batch[\"labels\"] = [label_info.str2int[label] for label in batch[\"labels\"]]\n",
    "        return tokenized_batch\n",
    "\n",
    "    # tokenize dataset\n",
    "    train_dataset = text_dataset['train'].map(tokenize, batched=True)\n",
    "    test_dataset = text_dataset['test'].map(tokenize, batched=True)\n",
    "\n",
    "    # set format for pytorch\n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "\n",
    "    class CustomTrainer(Trainer):\n",
    "        def compute_loss(self, model, inputs, return_outputs=False):\n",
    "            labels = inputs.get(\"labels\")\n",
    "            # forward pass\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.get(\"logits\")\n",
    "            # compute custom loss (suppose one has 3 labels with different weights)\n",
    "            loss_fct = nn.CrossEntropyLoss(weight=weights)\n",
    "            loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "            return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "    if device == \"cuda\":\n",
    "      fp_16 = True\n",
    "    else:\n",
    "      fp_16 = False\n",
    "\n",
    "    logging.info(\"Setting up Trainer Args\")\n",
    "    \n",
    "    output_dir = Path(\"/opt/ml/output/data\")\n",
    "        \n",
    "    logging.info(f'Created Directory')\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=output_dir.as_posix(),\n",
    "    num_train_epochs=args.epochs,\n",
    "    per_device_train_batch_size=args.train_batch_size,\n",
    "    per_device_eval_batch_size=args.eval_batch_size,\n",
    "    warmup_steps=args.warmup_steps,\n",
    "    fp16=fp_16,\n",
    "    learning_rate=float(args.learning_rate),\n",
    "    # logging & evaluation strategies\n",
    "    logging_dir=f\"{output_dir.as_posix()}/logs\",\n",
    "    logging_steps=50, \n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    report_to=\"wandb\",\n",
    "    # push to hub parameters\n",
    "    push_to_hub=args.push_to_hub,\n",
    "    hub_strategy=args.hub_strategy,\n",
    "    hub_model_id=args.hub_model_id,\n",
    "    hub_token=args.hub_token,\n",
    ")\n",
    "\n",
    "    # define data_collator\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    logging.info(\"Initializing Model\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    args.model_id, num_labels=number_classes, label2id=label2id, id2label=id2label\n",
    ")\n",
    "    trainer = CustomTrainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logging.info('Starting Model Training')\n",
    "    trainer.train()\n",
    "\n",
    "    logging.info('Computing Evaluation F1')\n",
    "    outputs = trainer.evaluate()\n",
    "    \n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(output_dir.as_posix(), \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(outputs.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "            print(f\"{key} = {value}\\n\")\n",
    "    logging.info(f'{outputs}')\n",
    "    logging.info(f\"Eval Accuracy: {outputs['eval_f1']}\")\n",
    "    logging.info('Saving Model')\n",
    "    trainer.save_model()\n",
    "    wandb.finish()\n",
    "\n",
    "    # save best model, metrics and create model card\n",
    "    if args.push_to_hub:\n",
    "        trainer.create_model_card(model_name=args.hub_model_id)\n",
    "        # wait for asynchronous pushes to finish\n",
    "        time.sleep(180)\n",
    "        trainer.push_to_hub()\n",
    "        \n",
    "    # Saves the model to s3 uses os.environ[\"SM_MODEL_DIR\"] to make sure checkpointing works\n",
    "    trainer.save_model(os.environ[\"SM_MODEL_DIR\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5214536c745168e902290bfd7f548ab7923537db143b81ee7a29a8e69c45f158"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
